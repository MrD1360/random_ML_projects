{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Experimentation \n",
    "\n",
    "This notebook is meant to be an experimentation on Sentiment Analysis with Deep Learning. Roughly speaking, to see if the task can be done using just the most frequently used words. \n",
    "\n",
    "Usually the order of the words is important for a better understanding of the meaning, and in this case of the sentiment of the processed sentence. Deep Learning models already proven to work efficiently in these cases.\n",
    "##### But what if we drop out the less frequently used words from the input sentence? \n",
    "Since all the words must be encoded and the dictionary dimension directly affects the number of trainable parameters in the networks, having less words would allow to use ligher models. \n",
    "\n",
    "### Methodology\n",
    "\n",
    "As dataset for training and testing it has been used a ready-to-use dataset provided by keras. More specifically the dataset contains IMDB reviews and a binary flag that says whether the review is good or bad.\n",
    "\n",
    "In this notebook I tried different networks:\n",
    "- GRU based network\n",
    "- Conv1D based network with squeeze and expansion layer\n",
    "- LSTM based network\n",
    "- Conv1D + GRU based network\n",
    "\n",
    "The best results are written down as comments in the cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if not tf.config.list_physical_devices('XLA_GPU'):\n",
    "    print(\"No GPU was detected.\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'XLA_GPU')\n",
    "\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the dataset\n",
    "(X_train,y_train),(X_test,y_test)= keras.datasets.imdb.load_data()\n",
    "\n",
    "\n",
    "#y=0 bad, y=1 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_word_index= keras.datasets.imdb.get_word_index()\n",
    "#dictionary_word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index to word dict\n",
    "\n",
    "def index_to_word(d):\n",
    "    d= {(index +3) : word for word,  index in d.items()}\n",
    "    d[0]='<pad>'\n",
    "    d[1]='<sos>'\n",
    "    d[2]='<unk>'\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_3(d):\n",
    "    d= {word : (index +3) for word,  index in d.items()}\n",
    "    d[0]='<pad>'\n",
    "    d[1]='<sos>'\n",
    "    d[2]='<unk>'\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep in the dictionary only the first \"threshold\" most frequent word, removing all the others from the dict\n",
    "\n",
    "def remove_less_freq(d, threshold):\n",
    "    '''\n",
    "    params: \n",
    "    d: dictionary\n",
    "    threshold: int \n",
    "    '''\n",
    "    d={index: word for index,word in d.items() if index <= (threshold+3)}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index to word dict\n",
    "dictionary_index_word=index_to_word(dictionary_word_index)\n",
    "dictionary_word_index=add_3(dictionary_word_index)\n",
    "#dictionary_index_word.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print first review\n",
    "#[dictionary_word_index[index] for index in X_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=30000 #threshold for frequent word\n",
    "x_t=300 #threshold for number of word for every review\n",
    "\n",
    "dictionary_index_word= remove_less_freq(dictionary_index_word,t)\n",
    "#dictionary_index_word.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing less frequent items also from X_train and zero padding it so they all have the same dimension\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]=[j for j in X_train[i] if j<(t+3)]\n",
    "    if len(X_train[i])>x_t:\n",
    "        X_train[i]=X_train[i][:x_t]\n",
    "    else:\n",
    "        X_train[i] += [0]*(x_t-len( X_train[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing items also from X_test and zero padding it\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i]=[j for j in X_test[i] if j<(t+3)]\n",
    "    if len(X_test[i])>x_t:\n",
    "        X_test[i]=X_test[i][:x_t]\n",
    "    else:\n",
    "        X_test[i] += [0]*(x_t-len( X_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform in numpy arrays\n",
    "\n",
    "X_train=np.array([np.array(xi) for xi in X_train]) \n",
    "X_test=np.array([np.array(xi) for xi in X_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for different exp\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D,Reshape,Multiply\n",
    "\n",
    "#squeeze and excite\n",
    "def sq_n_ex(input_, r=4):\n",
    "\n",
    "    '''\n",
    "    param: input , ratio \n",
    "    '''\n",
    "    input_sNe_shape = (1,input_.shape[2]) \n",
    "    sNe_layer = GlobalAveragePooling1D()(input_)\n",
    "    sNe_layer = Reshape(input_sNe_shape)(sNe_layer)\n",
    "    \n",
    "    #ratio is used only in the first fully connected layer\n",
    "    sNe_layer = Dense(input_.shape[2] // r, activation='relu', kernel_initializer='he_normal', use_bias=False)(sNe_layer)  \n",
    "    #hard sigmoid in the second FC\n",
    "    sNe_layer = Dense(input_.shape[2], activation='relu', kernel_initializer='he_normal', use_bias=False)(sNe_layer)\n",
    "    \n",
    "    return Multiply()([input_, sNe_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear keras session\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#model\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Conv1D, Concatenate,Input,Flatten,LSTM\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "'''\n",
    "# this model gets accuracy 0.85 with 30.000/300 as params rmsprop epoch 5\n",
    "model = keras.Sequential([\n",
    "    Embedding(t+3,128,mask_zero=True,input_shape=[None]),\n",
    "    GRU(128,return_sequences=True,dropout=0.2,recurrent_dropout=0.2),\n",
    "    GRU(128,dropout=0.2,recurrent_dropout=0.2),\n",
    "    Dense(128, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5)),\n",
    "    Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5), activation=\"sigmoid\")\n",
    "])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "The model overfits, params 40.000/200  / rmsprop\n",
    "accuracy with conc=0.83  epoch 5\n",
    "accuracy with one conv1d=0.84 epoch 5\n",
    "\n",
    "'''\n",
    "'''\n",
    "input_ =Input(shape=(x_t))\n",
    "em=Embedding(input_dim=t+3,output_dim=128,mask_zero=True)(input_)\n",
    "c1=Conv1D(64,1)(em)\n",
    "#c2=Conv1D(64,3,padding='same')(em)\n",
    "#c3=Conv1D(64,2,padding=\"same\")(em)\n",
    "se=sq_n_ex(c1)\n",
    "#conc= Concatenate()([se,c2])\n",
    "f=Flatten()(se)\n",
    "d1=Dense(128, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "            bias_regularizer=regularizers.l2(1e-4),\n",
    "            activity_regularizer=regularizers.l2(1e-5),activation=\"tanh\")(f)\n",
    "output_=Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "            bias_regularizer=regularizers.l2(1e-4),\n",
    "            activity_regularizer=regularizers.l2(1e-5), activation=\"sigmoid\")(d1)\n",
    "\n",
    "\n",
    "model=keras.Model(inputs=[input_],outputs=[output_])\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#3rd model, simple LTMS\n",
    "#params 10.000/200 accuracy 0.83 epoch 5\n",
    "input_ =Input(shape=(x_t))\n",
    "em=Embedding(input_dim=t+3,output_dim=128,mask_zero=True)(input_)\n",
    "\n",
    "l=LSTM(128,return_sequences=True)(em)\n",
    "l=LSTM(128)(l)\n",
    "\n",
    "output_=Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "            bias_regularizer=regularizers.l2(1e-4),\n",
    "            activity_regularizer=regularizers.l2(1e-5), activation=\"sigmoid\")(l)\n",
    "\n",
    "\n",
    "model=keras.Model(inputs=[input_],outputs=[output_])\n",
    "\n",
    "'''\n",
    "#4th model Conv1D+GRU\n",
    "#params 10.000/200 accuracy 0.81 epoch 8 patience 2\n",
    "#params 20.000/300 accuracy 0.839 epoch 5 patience 3\n",
    "model=keras.Sequential([\n",
    "    Embedding(input_dim=t+3,output_dim=128,mask_zero=True),\n",
    "    Conv1D(128,4,strides=2,padding='valid'),\n",
    "    GRU(128,return_sequences=True),\n",
    "    GRU(128,return_sequences=False),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         3840384   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         65664     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,104,321\n",
      "Trainable params: 4,104,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_a=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt_a, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 46s 296ms/step - loss: 0.6936 - accuracy: 0.5053 - val_loss: 0.6918 - val_accuracy: 0.5154\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 47s 301ms/step - loss: 0.6568 - accuracy: 0.5610 - val_loss: 0.6966 - val_accuracy: 0.5184\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 46s 293ms/step - loss: 0.4337 - accuracy: 0.7750 - val_loss: 0.4476 - val_accuracy: 0.8240\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 47s 299ms/step - loss: 0.1795 - accuracy: 0.9336 - val_loss: 0.4419 - val_accuracy: 0.8488\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 47s 299ms/step - loss: 0.0511 - accuracy: 0.9847 - val_loss: 0.4966 - val_accuracy: 0.8608\n"
     ]
    }
   ],
   "source": [
    "es=tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=5, batch_size=128,validation_split=0.2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 24s 31ms/step - loss: 0.5570 - accuracy: 0.8375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5570315718650818, 0.8375200033187866]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model.save('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "\n",
    "model = load_model('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new=[]\n",
    "sentence1=\"You've gotta love that sales pitch, and I find it hard to believe the director had trouble getting funding for Frantic But I really struggled with this film, and it comes down to pacing. Harrison Ford's rather stiff here, and the story's somewhat re-energized very once in a while with a new breadcrumb on the trail of his missing wife. This is essentially what concerns the movie's first half. Personally, I found a lot to like about Emmanuelle Seigner, and she really seemed to elevate her scenes with the star but she also comes in rather late in the game for such a key component.\".split()\n",
    "sentence2=\" His Bayisms were kept to a minimum, and the movie ran on the Smith/Lawrence chemistry, macho gun battles and slick polish. The Mark Mancina score added loads to the film, and it was pretty funny tosses out all of that. Everything is ramped to 11, including the camerawork, hateful dialogue and coked-fueled editing. This is a testament to a director whose id is fully in charge, and this saps all of the humor, fun and entertainment value It is exhausting.\".split()\n",
    "for word in sentence1:\n",
    "    if word in dictionary_word_index:\n",
    "        if(dictionary_word_index[word] in dictionary_index_word):\n",
    "            X_new.append(dictionary_word_index[word])\n",
    "if len(X_new)>x_t:\n",
    "    X_new=X_new[:x_t]\n",
    "else:\n",
    "    X_new += [0]*(x_t-len( X_new))\n",
    "X_new=np.array(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4854832]]\n",
      "better don't watch that movie!\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(X_new[None,...])\n",
    "print(pred)\n",
    "if pred>0.5:\n",
    "    print('That\\'s a good review!')\n",
    "else:\n",
    "    print('better don\\'t watch that movie!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In conclusion, besides overfitting (which can be fixed), the models show very low accuracy when dealing with new sentences with many words not included in the dictionary used in the training. Next step: add stopwords in order to get more meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

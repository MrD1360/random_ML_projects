{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Experimentation \n",
    "\n",
    "This notebook is meant to be an experimentation on Sentiment Analysis with Deep Learning. Roughly speaking, to see if the task can be done using just the most frequently used words. \n",
    "\n",
    "Usually the order of the words is important for a better understanding of the meaning, and in this case of the sentiment of the processed sentence. Deep Learning models already proven to work efficiently in these cases.\n",
    "##### But what if we drop out the less frequently used words from the input sentence? \n",
    "Since all the words must be encoded and the dictionary dimension directly affects the number of trainable parameters in the networks, having less words would allow to use ligher models. \n",
    "\n",
    "### Methodology\n",
    "\n",
    "As dataset for training and testing it has been used a ready-to-use dataset provided by keras. More specifically the dataset contains IMDB reviews and a binary flag that says whether the review is good or bad.\n",
    "\n",
    "In this notebook I tried different networks:\n",
    "- GRU based network\n",
    "- Conv1D based network with squeeze and expansion layer\n",
    "- LSTM based network\n",
    "- Conv1D + GRU based network\n",
    "\n",
    "The best results are written down as comments in the cell.\n",
    "#### Version2: \n",
    "- using nltk-stopwords and remove the stopwords among the \"most frequent\" ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if not tf.config.list_physical_devices('XLA_GPU'):\n",
    "    print(\"No GPU was detected.\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'XLA_GPU')\n",
    "\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the dataset\n",
    "(X_train,y_train),(X_test,y_test)= keras.datasets.imdb.load_data()\n",
    "#y=0 bad, y=1 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_word_index= keras.datasets.imdb.get_word_index()\n",
    "#dictionary_word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index to word dict\n",
    "\n",
    "def index_to_word(d):\n",
    "    d= {(index +3) : word for word,  index in d.items()}\n",
    "    d[0]='<pad>'\n",
    "    d[1]='<sos>'\n",
    "    d[2]='<unk>'\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_3(d):\n",
    "    d= {word : (index +3) for word,  index in d.items()}\n",
    "    d[0]='<pad>'\n",
    "    d[1]='<sos>'\n",
    "    d[2]='<unk>'\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep in the dictionary only the first \"threshold\" most frequent word, removing all the others from the dict\n",
    "\n",
    "def remove_less_freq(d, threshold):\n",
    "    '''\n",
    "    params: \n",
    "    d: dictionary\n",
    "    threshold: int \n",
    "    '''\n",
    "    d={index: word for index,word in d.items() if index <= (threshold+3)}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index to word dict\n",
    "dictionary_index_word=index_to_word(dictionary_word_index)\n",
    "dictionary_word_index=add_3(dictionary_word_index)\n",
    "#dictionary_index_word.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update\n",
    "\n",
    "- expand contraction forms\n",
    "- remove stop words\n",
    "- reduce dictionary dimensionality by removing stop-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "\"im\": \"i am\",\n",
    "\"dont\": \"do not\",\n",
    "\"doesnt\": \"does not\",\n",
    "\"theres\": \"there is\",\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \" what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_dict_out(sent):\n",
    "    enc=[]\n",
    "    for word in sent.lower().split():\n",
    "        enc.append(dictionary_word_index[word])\n",
    "    return enc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))  \n",
    "stop_words.add('br')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')  #removing this in order to differentiate \"good\" from \"not good\"\n",
    "stop_words.remove('nor')\n",
    "\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    new_X=[]\n",
    "    for index in X_train[i]:\n",
    "        if dictionary_index_word[index] in contractions_dict:\n",
    "            new_X.extend(encoder_dict_out(contractions_dict[dictionary_index_word[index]]))\n",
    "        else:\n",
    "            new_X.append(index)\n",
    "    \n",
    "    X_train[i]=new_X[0:]\n",
    "\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = [index for index in X_train[i] if not dictionary_index_word[index] in stop_words]\n",
    "        \n",
    "for i in range(len(y_train)):\n",
    "    X_test[i] = [index for index in X_test[i] if not dictionary_index_word[index] in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>\n",
      "french\n",
      "horror\n",
      "cinema\n",
      "seen\n",
      "something\n",
      "revival\n",
      "last\n",
      "couple\n",
      "years\n",
      "great\n",
      "films\n",
      "inside\n",
      "switchblade\n",
      "romance\n",
      "bursting\n",
      "scene\n",
      "maléfique\n",
      "preceded\n",
      "revival\n",
      "slightly\n",
      "stands\n",
      "head\n",
      "shoulders\n",
      "modern\n",
      "horror\n",
      "titles\n",
      "surely\n",
      "one\n",
      "best\n",
      "french\n",
      "horror\n",
      "films\n",
      "ever\n",
      "made\n",
      "maléfique\n",
      "obviously\n",
      "shot\n",
      "low\n",
      "budget\n",
      "made\n",
      "far\n",
      "ways\n",
      "one\n",
      "originality\n",
      "film\n",
      "turn\n",
      "complimented\n",
      "excellent\n",
      "writing\n",
      "acting\n",
      "ensure\n",
      "film\n",
      "winner\n",
      "plot\n",
      "focuses\n",
      "two\n",
      "main\n",
      "ideas\n",
      "prison\n",
      "black\n",
      "magic\n",
      "central\n",
      "character\n",
      "man\n",
      "named\n",
      "carrère\n",
      "sent\n",
      "prison\n",
      "fraud\n",
      "put\n",
      "cell\n",
      "three\n",
      "others\n",
      "quietly\n",
      "insane\n",
      "lassalle\n",
      "body\n",
      "building\n",
      "transvestite\n",
      "marcus\n",
      "retarded\n",
      "boyfriend\n",
      "daisy\n",
      "short\n",
      "cell\n",
      "together\n",
      "stumble\n",
      "upon\n",
      "hiding\n",
      "place\n",
      "wall\n",
      "contains\n",
      "old\n",
      "journal\n",
      "translating\n",
      "part\n",
      "soon\n",
      "realise\n",
      "magical\n",
      "powers\n",
      "realise\n",
      "may\n",
      "able\n",
      "use\n",
      "break\n",
      "prison\n",
      "walls\n",
      "black\n",
      "magic\n",
      "interesting\n",
      "topic\n",
      "actually\n",
      "quite\n",
      "surprised\n",
      "not\n",
      "films\n",
      "based\n",
      "much\n",
      "scope\n",
      "things\n",
      "fair\n",
      "say\n",
      "maléfique\n",
      "makes\n",
      "best\n",
      "assets\n",
      "despite\n",
      "restraints\n",
      "film\n",
      "never\n",
      "actually\n",
      "feels\n",
      "restrained\n",
      "manages\n",
      "flow\n",
      "well\n",
      "throughout\n",
      "director\n",
      "eric\n",
      "valette\n",
      "provides\n",
      "great\n",
      "atmosphere\n",
      "film\n",
      "fact\n",
      "takes\n",
      "place\n",
      "inside\n",
      "central\n",
      "prison\n",
      "cell\n",
      "ensures\n",
      "film\n",
      "feels\n",
      "claustrophobic\n",
      "immensely\n",
      "benefits\n",
      "central\n",
      "idea\n",
      "prisoners\n",
      "wanting\n",
      "use\n",
      "magic\n",
      "break\n",
      "cell\n",
      "easy\n",
      "get\n",
      "behind\n",
      "often\n",
      "said\n",
      "unknown\n",
      "thing\n",
      "really\n",
      "frightens\n",
      "people\n",
      "film\n",
      "proves\n",
      "director\n",
      "ensures\n",
      "never\n",
      "really\n",
      "sure\n",
      "exactly\n",
      "round\n",
      "corner\n",
      "helps\n",
      "ensure\n",
      "maléfique\n",
      "actually\n",
      "manage\n",
      "quite\n",
      "frightening\n",
      "film\n",
      "memorable\n",
      "lot\n",
      "reasons\n",
      "outside\n",
      "central\n",
      "plot\n",
      "characters\n",
      "interesting\n",
      "way\n",
      "fact\n",
      "book\n",
      "almost\n",
      "takes\n",
      "character\n",
      "well\n",
      "done\n",
      "anyone\n",
      "worried\n",
      "film\n",
      "not\n",
      "deliver\n",
      "end\n",
      "not\n",
      "disappointed\n",
      "either\n",
      "ending\n",
      "makes\n",
      "sense\n",
      "manages\n",
      "quite\n",
      "horrifying\n",
      "overall\n",
      "maléfique\n",
      "truly\n",
      "great\n",
      "horror\n",
      "film\n",
      "one\n",
      "best\n",
      "decade\n",
      "highly\n",
      "recommended\n",
      "viewing\n"
     ]
    }
   ],
   "source": [
    "#print first review before removing words\n",
    "#[dictionary_index_word[index] for index in X_train[10]]\n",
    "for x in X_train[10]:\n",
    "    print(dictionary_index_word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'br',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print first review with stop words removed\n",
    "#[dictionary_index_word[index] for index in X_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_dict(d_i2w,d_w2i):\n",
    "    for word in stop_words:\n",
    "        if word in dictionary_word_index:\n",
    "            index_word=dictionary_word_index[word]\n",
    "            if index_word in d_i2w:\n",
    "                del d_i2w[index_word]\n",
    "                del d_w2i[word]\n",
    "    return d_i2w,d_w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_index_word,dictionary_word_index=cleaning_dict(dictionary_index_word,dictionary_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=30000 #threshold for frequent word\n",
    "x_t=300 #threshold for number of word for every review\n",
    "\n",
    "dictionary_index_word= remove_less_freq(dictionary_index_word,t)\n",
    "#dictionary_index_word.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing less frequent items also from X_train and zero padding it so they all have the same dimension\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]=[j for j in X_train[i] if j<(t+3)]\n",
    "    if len(X_train[i])>x_t:\n",
    "        X_train[i]=X_train[i][:x_t]\n",
    "    else:\n",
    "        X_train[i] += [0]*(x_t-len( X_train[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing items also from X_test and zero padding it\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i]=[j for j in X_test[i] if j<(t+3)]\n",
    "    if len(X_test[i])>x_t:\n",
    "        X_test[i]=X_test[i][:x_t]\n",
    "    else:\n",
    "        X_test[i] += [0]*(x_t-len( X_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform in numpy arrays\n",
    "\n",
    "X_train=np.array([np.array(xi) for xi in X_train]) \n",
    "X_test=np.array([np.array(xi) for xi in X_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for different exp\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D,Reshape,Multiply\n",
    "\n",
    "#squeeze and excite\n",
    "def sq_n_ex(input_, r=4):\n",
    "\n",
    "    '''\n",
    "    param: input , ratio \n",
    "    '''\n",
    "    input_sNe_shape = (1,input_.shape[2]) \n",
    "    sNe_layer = GlobalAveragePooling1D()(input_)\n",
    "    sNe_layer = Reshape(input_sNe_shape)(sNe_layer)\n",
    "    \n",
    "    #ratio is used only in the first fully connected layer\n",
    "    sNe_layer = Dense(input_.shape[2] // r, activation='relu', kernel_initializer='he_normal', use_bias=False)(sNe_layer)  \n",
    "    #hard sigmoid in the second FC\n",
    "    sNe_layer = Dense(input_.shape[2], activation='relu', kernel_initializer='he_normal', use_bias=False)(sNe_layer)\n",
    "    \n",
    "    return Multiply()([input_, sNe_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#3rd model, simple LTMS\\n#params 10.000/200 accuracy 0.83 epoch 5\\ninput_ =Input(shape=(x_t))\\nem=Embedding(input_dim=t+3,output_dim=128,mask_zero=True)(input_)\\n\\nl=LSTM(128,return_sequences=True)(em)\\nl=LSTM(128)(l)\\n\\noutput_=Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\\n            bias_regularizer=regularizers.l2(1e-4),\\n            activity_regularizer=regularizers.l2(1e-5), activation=\"sigmoid\")(l)\\n\\n\\nmodel=keras.Model(inputs=[input_],outputs=[output_])\\n\\n'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clear keras session\n",
    "K.clear_session()\n",
    "\n",
    "#model\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Conv1D, Concatenate,Input,Flatten,LSTM\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "# this model gets accuracy 0.85 with 30.000/300 as params rmsprop epoch 5\n",
    "latent_dim=64\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Embedding(t+3,latent_dim,mask_zero=True,input_shape=[None]),\n",
    "    GRU(latent_dim,return_sequences=True,dropout=0.4,recurrent_dropout=0.4),\n",
    "    GRU(latent_dim,dropout=0.4,recurrent_dropout=0.4),\n",
    "    Dense(latent_dim, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5)),\n",
    "    Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5), activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#3rd model, simple LTMS\n",
    "#params 10.000/200 accuracy 0.83 epoch 5\n",
    "input_ =Input(shape=(x_t))\n",
    "em=Embedding(input_dim=t+3,output_dim=128,mask_zero=True)(input_)\n",
    "\n",
    "l=LSTM(128,return_sequences=True)(em)\n",
    "l=LSTM(128)(l)\n",
    "\n",
    "output_=Dense(1,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "            bias_regularizer=regularizers.l2(1e-4),\n",
    "            activity_regularizer=regularizers.l2(1e-5), activation=\"sigmoid\")(l)\n",
    "\n",
    "\n",
    "model=keras.Model(inputs=[input_],outputs=[output_])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          1920192   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 64)          24960     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                24960     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,974,337\n",
      "Trainable params: 1,974,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_a=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt_a, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 61s 386ms/step - loss: 0.4402 - accuracy: 0.7913 - val_loss: 0.3028 - val_accuracy: 0.8796\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 62s 397ms/step - loss: 0.1951 - accuracy: 0.9329 - val_loss: 0.3398 - val_accuracy: 0.8794\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 62s 393ms/step - loss: 0.1031 - accuracy: 0.9668 - val_loss: 0.3573 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "es=tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "history = model.fit(X_train,y_train, epochs=5, batch_size=128,validation_split=0.2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 33s 42ms/step - loss: 0.4017 - accuracy: 0.8569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40172043442726135, 0.8568800091743469]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model.save('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "\n",
    "model = load_model('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(sentence):\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    sentence=sentence.lower()\n",
    "    sentence = sentence.translate(remove_digits)\n",
    "    sentence=sentence.translate(table).lower() #remove punt and set to lower\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_feedback(sentence):\n",
    "    X_new=[]\n",
    "    print(sentence, \"\\t\")\n",
    "    sentence=preprocess_input(sentence)\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word in dictionary_word_index:\n",
    "            if(dictionary_word_index[word] in dictionary_index_word):\n",
    "                X_new.append(dictionary_word_index[word])\n",
    "    if len(X_new)>x_t:\n",
    "        X_new=X_new[:x_t]\n",
    "    else:\n",
    "        X_new += [0]*(x_t-len( X_new))\n",
    "    \n",
    "    X_new=np.array(X_new)\n",
    "    \n",
    "    pred=model.predict(X_new[None,...])\n",
    "\n",
    "    if pred>0.5:\n",
    "        print('That\\'s a good review! \\n')\n",
    "    else:\n",
    "        print('better don\\'t watch that movie! \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was awesome \t\n",
      "That's a good review! \n",
      "\n",
      "i hate this movie, it is terrible \t\n",
      "better don't watch that movie! \n",
      "\n",
      "this is not good, hate it \t\n",
      "That's a good review! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence1=\"This movie was awesome\"\n",
    "sentence2=\"i hate this movie, it is terrible\"\n",
    "sentence3='this is not good, hate it'  ##!!!! \n",
    "\n",
    "predict_feedback(sentence1)\n",
    "predict_feedback(sentence2)\n",
    "predict_feedback(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Some consideration of this second version:\n",
    "- stop-words work like noise, removing them better results are obtained\n",
    "- Conv works terrebly and the task is better perform by GRU cells\n",
    "- Purposely removing \"not/no/nor\" from the list of stopword, in order to have the relation \"good\"(positive) \"not good\"(negative), do not work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
